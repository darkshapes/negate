---
language:
  - en
library_name: nnll
license_name: MPL-2.0 + Commons Clause 1.0
---

<div align="center">

![Stylized futuristic lines in the shape of an N](https://raw.githubusercontent.com/darkshapes/entity-statement/refs/heads/main/png/negate/negate_150.png)</div>

# negate <br><sub> entrypoint synthetic image classifier</sub>

A CLI tool and Python library for image processing and analysis of feature extraction to determine origin, whether synthetic/reconstructed or genuine.

[![negate pytest](https://github.com/darkshapes/negate/actions/workflows/negate.yml/badge.svg?branch=main)](https://github.com/darkshapes/negate/actions/workflows/negate.yml)<br>
[<img src="https://img.shields.io/discord/1266757128249675867?color=5865F2">](https://discord.gg/VVn9Ku74Dk)<br>
[<img src="https://img.shields.io/badge/me-__?logo=kofi&logoColor=white&logoSize=auto&label=feed&labelColor=maroon&color=grey&link=https%3A%2F%2Fko-fi.com%2Fdarkshapes">](https://ko-fi.com/darkshapes)<br>
<br>

## Overview

<div align="center">

<img src="results/visualization_20260209_101743.svg" style="width:50%; max-width:500px;" alt="Visualization of model results for the DinoViTL Model"></div>

Negate is a modular system of image processing and feature extraction pipelines aimed towards critical analysis of effective techniques that can differentiate synthetic and human-origin works. Techniques include spectral residual analysis, laplacian residual analysis, random resize crop, local binary pattern and haar wavelet perturbation. We also study the efficacy of a gradient-boosted decision tree and PCA trained to distinguish between images of synthetic and human origin. Preliminary results demonstrate high accuracy in detecting synthetic images.

This repo provides a simple commandâ€‘line interface to invoke classification and examples of integrating our library of training scripts, predictions and metrics into other works. We make all attempts to follow continuous integration best practices for deploying and maintaining software, ensuring the code is readied for production environments.

Future work includes the development of an automated testing framework and evaluation suite, expanding the scope of research to include wider diversity of synthetic and original human-generated datasets, benchmarking against comparable methods, and exploring additional model architectures.

Results were obtained using a curated dataset of approximately 2000 images of known origin. Synthetic images have been generated by Diffusers, ComfyUI, Darkshapes tools (Zodiac/Divisor/singularity), Google Nano-Banana, Midjourney, and Seedream, while art authored by humans was supplied by consent from artist volunteers.

> [!NOTE]
>
> Our training results and visualizations were created with data provided consensually by generous artists at https://purelyhuman.xyz. We don't have and won't seek permission to share their art here.

## Requirements

- A dataset of images made by human artists with a minimum width and height of 512 pixels but preferably 1024px or greater. This will serve as ground truth and should be placed in the `/assets` folder.
- A [huggingface](https://hf.co) account that will be used to download models and synthetic datasets. Create an API Key at their website, then sign in with `hf auth login`. Some datasets may use .ZIP files which should be extracted to local folders.
- It is recommended to run `negate` on a machine with a GPU to ensure efficient processing and reduced training time.

## Install

[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![pytest](https://img.shields.io/badge/logo-pytest-blue?logo=pytest&labelColor=5c5c5c&label=%20)](https://github.com/pytest-dev/pytest)<br>

```bash
git clone https://github.com/darkshapes/negate.git
cd negate
uv sync
```

<sub>macos/linux</sub>

```bash
source .venv/bin/activate
```

<sub>windows</sub>

```powershell
Set-ExecutionPolicy Bypass -Scope Process -Force; .venv\Scripts\Activate.ps1
```

## Default Folder Locations and Configuration Options

- [.datasets](.datasets) : For downloaded datasets. (HuggingFace Hub)
- [/assets](/assets) : For local datasets.
- [/models](/models) : For downloaded models (HuggingFace Hub)
- [/models](/models)/`YYMMDD_HHMMSS` Models trained and exported by `negate` are placed in dated subfolders of `/models`.
- [/results](/results): For training and inference result graphs.
- [config/config.toml](config/config.toml) specifies all the data, model, training and inference parameters above. Main commands and additional paths or models may be chosen at runtime using the CLI.

## Using the CLI:

Basic Syntax:

```bash
usage: negate [-h] {calibrate,train} ...

Negate CLI

positional arguments:
  {calibrate,train}
    calibrate        Check model on the dataset at the provided path from CLI or config, default `assets/`.
    train            Train XGBoost model on wavelet features using the dataset in the provided path or `assets/`. The resulting model will be
                     saved to disk.

options:
  -h, --help         show this help message and exit
```

Evaluating the haar-wavelet method:

```bash
usage: negate calibrate [-h]
                        [-m {['nvidia/C-RADIOv4-SO400M', 'facebook/dinov3-vitl16-pretrain-sat493m'],['timm/vit_base_patch16_dinov3.lvd1689m'],['timm/MobileCLIP2-S4-OpenCLIP']}]
                        [path]

positional arguments:
  path                  Genunie/Human-original dataset path

options:
  -h, --help            show this help message and exit
  -m, --model {['nvidia/C-RADIOv4-SO400M', 'facebook/dinov3-vitl16-pretrain-sat493m'],['timm/vit_base_patch16_dinov3.lvd1689m'],['timm/MobileCLIP2-S4-OpenCLIP']}
                        Model to use. Default :nvidia/C-RADIOv4-SO400M
```

Training a model to distinguish genuine from synthetic art:

```bash
usage: negate train [-h] [path]

positional arguments:
  path        Genunie/Human-original dataset path

options:
  -h, --help  show this help message and exit
```

## Fourier Feature Comparison by Model

<img src="results/visualization_20260209_101743.svg" style="width:50%; max-width:500px;" alt="Visualization of model results for the TIMM DinoV3_Base Model">
<img src="results/visualization_20260208_1151.svg" style="width:50%; max-width:500px;" alt="Visualization of model results for the MobileClip model">
<img src="results/visualization_20260208_222311.svg" style="width:50%; max-width:500px;" alt="Visualization of model results for the Nvidia C-RADIOv4-SO400M Model">
<img src="results/combined_plots.png" style="width:50%; max-width:500px;" alt="Bar and grid graph comparing variance of the synthetic and real images">

## Related Research:

- [arxiv:2511.14030](https://arxiv.org/abs/2511.14030)
- [arxiv:2505.11278](https://arxiv.org/abs/2505.11278)
- [arxiv:2502.15176](https://arxiv.org/abs/2502.15176)
- [arxiv:2411.19417](https://arxiv.org/abs/2411.19417)
- [arxiv:2409.07913](https://arxiv.org/abs/2409.07913)
- [VeridisQuo](https://github.com/VeridisQuo-orga/VeridisQuo)
- https://drive.google.com/file/d/1bxDmREBn-TkTe-GkcLVD9_bVw3nokehI/view

```bib
@misc{darkshapes2026,
    author={darkshapes},
    title={negate},
    year={2026},
    primaryClass={cs.CV},
    howpublished={\url={https://github.com/darkshapes/negate}},
}
```
